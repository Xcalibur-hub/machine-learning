{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Plant_Disease_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "L4uMifr__aDc",
        "C6tmATL4_ecG",
        "NuGK0vnQ_jRe",
        "EHhlQnxs_SXv",
        "EnyAcVHQCmUG",
        "CE9qVDQTFEct",
        "w9uxSFjxMY9w",
        "HgwxOoX0EQhl",
        "M_ogK7rSCKCj",
        "YybBG0FDCCMQ",
        "euXbHlTReseg",
        "wumWWhLB6gzc",
        "XluDawQiSBbc",
        "aq8erty27Ip2",
        "LS9_tKl-e1He"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xcalibur-hub/machine-learning/blob/main/Plant_Disease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from keras.applications import VGG16, VGG19, InceptionV3, ResNet50\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def count_images(directory):\n",
        "    return sum(len(files) for _, _, files in os.walk(directory))\n",
        "\n",
        "def prepare_data(train_dir, test_dir, target_size=(224, 224), batch_size=32):\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(train_dir, target_size=target_size, batch_size=batch_size)\n",
        "    test_generator = test_datagen.flow_from_directory(test_dir, target_size=target_size, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "    return train_generator, test_generator\n",
        "\n",
        "def build_model(base_model, num_classes):\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "    return Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "def train_model(model, train_generator, validation_generator, epochs=5):\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator,\n",
        "                        callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3)])\n",
        "    return history\n",
        "\n",
        "# Paths\n",
        "train_dir = \"/content/drive/MyDrive/Colab Notebooks/dataset/Plant Leafs/train\"\n",
        "test_dir = \"/content/drive/MyDrive/Colab Notebooks/dataset/Plant Leafs/val\"\n",
        "\n",
        "# Image counts\n",
        "num_classes = len(glob.glob(train_dir + \"/*\"))\n",
        "train_samples = count_images(train_dir)\n",
        "test_samples = count_images(test_dir)\n",
        "\n",
        "print(f\"{num_classes} Classes\")\n",
        "print(f\"{train_samples} Train images\")\n",
        "print(f\"{test_samples} Test images\")\n",
        "\n",
        "# Prepare data\n",
        "train_generator, test_generator = prepare_data(train_dir, test_dir)\n",
        "\n",
        "# Build and train models\n",
        "base_models = [VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "               VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "               InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n",
        "               ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))]\n",
        "\n",
        "for base_model in base_models:\n",
        "    model = build_model(base_model, num_classes)\n",
        "    history = train_model(model, train_generator, test_generator)\n",
        "\n",
        "# Save model example\n",
        "model.save('/content/drive/MyDrive/Colab Notebooks/Model/my_model.h5')\n",
        "\n",
        "# Load and predict example\n",
        "def prepare_img(img_path):\n",
        "    img = load_img(img_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img) / 255.0\n",
        "    return np.expand_dims(img_array, axis=0)\n",
        "\n",
        "img_url = '/path/to/image.jpg'\n",
        "model = load_model('/content/drive/MyDrive/Colab Notebooks/Model/my_model.h5')\n",
        "result = model.predict(prepare_img(img_url))\n",
        "predicted_class = np.argmax(result)\n",
        "print(f\"Predicted class: {predicted_class}\")\n"
      ],
      "metadata": {
        "id": "XFNQS6V1rGi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}